replicaCount: 1

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  annotations: {}
  hosts:
    - host: chart-example.local
      paths: []
  tls: []

resources: {}

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

# Values: Kafka

kafka:
  ## Kafka Brokers The StatefulSet installs 3 pods by default
  replicas: 1

  ## The kafka image repository
  image: "confluentinc/cp-kafka"

  ## The kafka image tag
  imageTag: "5.0.1"  # Confluent image for Kafka 2.0.0

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: "IfNotPresent"

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
  kafkaHeapOptions: "-Xmx1G -Xms1G"

  ## Optional Container Security context
  securityContext: {}

  ## The StatefulSet Update Strategy which Kafka will use when changes are applied.
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  updateStrategy:
    type: "RollingUpdate"

  ## Start and stop pods in Parallel or OrderedReady (one-by-one.)  Note - Can not change after first release.
  ## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
  podManagementPolicy: OrderedReady

  logSubPath: "logs"

  affinity: {}

  nodeSelector: {}

  ## Readiness probe config.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
  ##
  readinessProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 3

  ## Period to wait for broker graceful shutdown (sigterm) before pod is killed (sigkill)
  ## ref: https://kubernetes-v1-4.github.io/docs/user-guide/production-pods/#lifecycle-hooks-and-termination-notice
  ## ref: https://kafka.apache.org/10/documentation.html#brokerconfigs controlled.shutdown.*
  terminationGracePeriodSeconds: 60

  # Tolerations for nodes that have taints on them.
  # Useful if you want to dedicate nodes to just run kafka
  # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  # tolerations:
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"

  ## Headless service.
  ##
  headless:
    # annotations:
    # targetPort:
    port: 9092

  ## External access.
  ##
  external:
    enabled: false
    # type can be either NodePort or LoadBalancer
    type: NodePort
    # annotations:
    #  service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
    dns:
      useInternal: false
      useExternal: true
    # If using external service type LoadBalancer and external dns, set distinct to true below.
    # This creates an A record for each statefulset pod/broker. You should then map the
    # A record of the broker to the EXTERNAL IP given by the LoadBalancer in your DNS server.
    distinct: false
    servicePort: 19092
    firstListenerPort: 31090
    domain: cluster.local
    loadBalancerIP: []
    loadBalancerSourceRanges: []
    init:
      image: "lwolf/kubectl_deployer"
      imageTag: "0.4"
      imagePullPolicy: "IfNotPresent"

  # Annotation to be added to Kafka pods
  podAnnotations: {}

  # Labels to be added to Kafka pods
  podLabels: {}
    # service: broker
    # team: developers

  podDisruptionBudget: {}

  configurationOverrides:
    "confluent.support.metrics.enable": true  # Disables confluent metric submission
    # "auto.leader.rebalance.enable": true
    "auto.create.topics.enable": true

  envOverrides: {}


  ## A collection of additional ports to expose on brokers (formatted as normal containerPort yaml)
  # Useful when the image exposes metrics (like prometheus, etc.) through a javaagent instead of a sidecar
  additionalPorts: {}

  ## Persistence configuration. Specify if and how to persist data to a persistent volume.
  ##
  persistence:
    enabled: true

    size: "1Gi"

    mountPath: "/opt/kafka/data"

  jmx:
    
    configMap:

      ## Allows disabling the default configmap, note a configMap is needed
      enabled: true

      ## Allows setting values to generate confimap
      ## To allow all metrics through (warning its crazy excessive) comment out below `overrideConfig` and set
      ## `whitelistObjectNames: []`
      overrideConfig: {}

      overrideName: ""

    ## Port the jmx metrics are exposed in native jmx format, not in Prometheus format
    port: 5555

    ## JMX Whitelist Objects, can be set to control which JMX metrics are exposed.  Only whitelisted
    ## values will be exposed via JMX Exporter.  They must also be exposed via Rules.  To expose all metrics
    ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`
    ## (2) commented out above `overrideConfig`.
    whitelistObjectNames:  # []
    - kafka.controller:*
    - kafka.server:*
    - java.lang:*
    - kafka.network:*
    - kafka.log:*

  ## Prometheus Exporters / Metrics
  ##
  prometheus:
    ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics
    jmx:
      enabled: true

      ## The image to use for the metrics collector
      image: solsson/kafka-prometheus-jmx-exporter@sha256

      ## The image tag to use for the metrics collector
      imageTag: a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8

      ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
      interval: 10s

      ## Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
      scrapeTimeout: 10s

      ## Port jmx-exporter exposes Prometheus format metrics to scrape
      port: 5556

      resources: {}
        # limits:
        #   cpu: 200m
        #   memory: 1Gi
        # requests:
        #   cpu: 100m
        #   memory: 100Mi

    
    kafka:
      enabled: true

      ## The image to use for the metrics collector
      image: danielqsj/kafka-exporter

      ## The image tag to use for the metrics collector
      imageTag: v1.2.0

      ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
      interval: 10s

      ## Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
      scrapeTimeout: 10s

      ## Port kafka-exporter exposes for Prometheus to scrape metrics
      port: 9308

      ## Resource limits
      resources: {}

      tolerations: []

      affinity: {}

      nodeSelector: {}

    operator:
      ## Are you using Prometheus Operator?
      enabled: false

      serviceMonitor:
        # Namespace in which to install the ServiceMonitor resource.
        namespace: monitoring
        #namespace: default
        # Use release namespace instead
        releaseNamespace: false

        ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
        ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
        ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
        selector:
          prometheus: kube-prometheus

      prometheusRule:
        ## Add Prometheus Rules?
        enabled: false

        ## Namespace in which to install the PrometheusRule resource.
        namespace: monitoring
        #namespace: default
        # Use release namespace instead
        releaseNamespace: false

        selector:
          prometheus: kube-prometheus

        rules:
        - alert: KafkaNoActiveControllers
          annotations:
            message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is less than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
          expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) < 1
          for: 5m
          labels:
            severity: critical
        - alert: KafkaMultipleActiveControllers
          annotations:
            message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is greater than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
          expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) > 1
          for: 5m
          labels:
            severity: critical


  configJob:
    backoffLimit: 6

  topics: 
    - name: topic01-team3
      partitions: 3
      replicationFactor: 2
      reassignPartitions: true

  testsEnabled: true


# Values: Zookeepeer

zookeeper:
  replicaCount: 1
  ## If true, install the Zookeeper chart alongside Kafka
  ## ref: https://github.com/kubernetes/charts/tree/master/incubator/zookeeper
  enabled: true

  ## Configure Zookeeper resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: ~

  ## Environmental variables to set in Zookeeper
  env:
    ## The JVM heap size to allocate to Zookeeper
    ZK_HEAP_SIZE: "1G"

  persistence:
    enabled: false
    ## The amount of PV storage allocated to each Zookeeper pod in the statefulset
    # size: "2Gi"

  ## Specify a Zookeeper imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  image:
    PullPolicy: "IfNotPresent"

  ## If the Zookeeper Chart is disabled a URL and port are required to connect
  url: ""
  port: 2181

  ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## By default we don't set affinity:
  affinity: {}  # Criteria by which pod label-values influence scheduling for zookeeper pods.

# Values: Elastic Search

elasticsearch:

  replicas: 2
  minimumMasterNodes: 1
  clusterHealthCheckParams: "wait_for_status=green&timeout=60s"
  clusterName: esinfra
  esJavaOpts: "-Xmx1g -Xms1g"

  resources:
    requests:
      cpu: "550m"
      memory: "1Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  podManagementPolicy: "Parallel"

  # This is the max unavailable setting for the pod disruption budget
  # The default value of 1 will make sure that kubernetes won't allow more than 1
  # of your pods to be unavailable during maintenance
  maxUnavailable: 1

  readinessProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 3

# Values: Kibana

kibana:

  replicas: 1

  elasticsearchHosts: "http://esinfra-master:9200"

  resources:
    requests:
      cpu: "550m"
      memory: "1Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  updateStrategy:
    type: "RollingUpdate"

  readinessProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 3

    
# Values: Fluentd 

fluentd-elasticsearch:
  elasticsearch:
    hosts: ["esinfra-master:9200"]
    host: 'esinfra-master'
    port: 9200

# Values: Prometheus

prometheus:
  server:
    port: 5601
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
      scrape_timeout: 10s

  
  serverFiles:

    prometheus.yml:
      rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml
      ## Below two files are DEPRECATED will be removed from this default values file
        - /etc/config/rules
        - /etc/config/alerts

      scrape_configs:
        - job_name: webapp
          scrape_interval: 10s
          scrape_timeout: 10s
          honor_labels: true
          metrics_path: /actuator/prometheus
          static_configs:
            - targets: ['app-csye7125:8080'] 

        - job_name: eexporter          
          scrape_interval: 10s
          scrape_timeout: 10s  
          honor_labels: true
          static_configs:
            - targets: ['infra-prometheus-elasticsearch-exporter:9108']

        - job_name: kafka-exporter
          scrape_interval: 10s
          scrape_timeout: 10s  
          honor_labels: true
          static_configs:
            - targets: ['kafka-exporter:9308']       
                    
                        
#Values: grafana

grafana:
  service:
    port: 3000

  rbac:
    create: false

